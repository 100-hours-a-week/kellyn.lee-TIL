# 25.02.22.Sat - TIL

### [오늘 할 일]

- 데일리 스크럼
- 금일 강의 내용 학습 (복습) 및 정리 + 실습 진행
     - LLM 호출 및 연결 (Langchain & OpenAI 사용)
     - Langchain LLM + FastAPI server 연결
     - LangGraph를 이용한 챗봇 만들기
- 4주차 과제 진행

---
 
### [LangChain & LangGraph]

- 학습 (실습) 내용에 대한 GitHub
     - https://github.com/kellyn0522/api-server
     - 코드에 대한 학습 내용은 위의 링크를 통해서 정리
 
- 추가적인 개념 및 내용 정리 아래 노션 페이지 참고
     - https://aboard-teeth-ea5.notion.site/25-02-22-LangChain-LangGraph-1a2a0c19894d80fb947fd4b6775f309f?pvs=4

---

### [LangChain에서 데이터를 LLM이 아니라 LangGraph의 메모리에 저장해야 하는 이유]

- LLM이 아니라 랭그래프의 메모리를 통해서 데이터를 자신이 가지고 있어야 함.
- GPT와 대화할 때 대화가 길어지는 이유도 이와 동일

1. LLM은 **상태를 유지하지 못함**
    1) LLM은 기본적으로 상태를 유지하지 않는 **비결정적 함수**와 비슷
    2) 한 번의 프롬프트가 들어올 때마다 **완전히 독립적인 응답을 생성**
    3) 과거의 문맥을 저장하고 있지 않음
    
    → 따라서, LLM에 이전 데이터를 직접 저장하려면 **매번 모든 데이터를 포함한 긴 프롬프트를 보내야함** : 이 경우 토큰 제한이 걸리고 응답 속도가 느려짐
    
2. 대화 기록이 길어질수록 **LLM 응답이 느려짐**
    1) LLM은 과거의 대화 내역을 직접 기억하지 못하기 때문에 매번 모든 과거 기록을 포함한 긴 프롬프트를 보내야 함.
    
    → 대화가 길어질수록 모델이 처리해야 할 **입력 토큰 수가 늘어나면서 연산 비용이 증가하고, 응답 속도도 느려지는 현상이 생김** 


   [랭그래프의 메모리 사용이 필요한 이유]

    - LLM이 아니라 **로컬 메모리나 DB에 상태를 저장하고 필요할 때만 적절한 컨텍스트를 불러올 수 있음**
         - **효율적인 상태 관리** : 이전 데이터를 필요할 때만 불러와서 LLM 부담 줄임
         - **응답 속도 개선** : 매번 모든 데이터를 보내지 않고, 필요한 정보만 LLM에 전달
         - **토큰 제한 문제 해결** : 모든 데이터를 포함한 긴 프롬프트를 보내지 않으므로 컨텍스트 윈도우 초과 방지
