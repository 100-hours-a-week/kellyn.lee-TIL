# 25.02.17.Mon - TIL

### [오늘 할 일]

- 데일리 스크럼 진행
- 보충 심화 강의 : 다변수 미적분
     - 금일 강의 내용 복습(학습) 및 정리
- 3주차 미니퀘스트 완료
- 3주차 github TIL 정리
- 3주차 과제 진행 및 제출 : 미니퀘스트, github TIL, 위클리 챌린지, 회고록
- AI 맛보기 : 유튜브를 통해서 간단하게 학습
- 조건부 확률 추가 학습

---

### [다변수 미분 강의]

- 정리된 내용에 대해서 노션 링크를 통해서 확인 가능 (아래 링크)
     - https://aboard-teeth-ea5.notion.site/25-02-17-19da0c19894d80c3a559d972c87b5fe7?pvs=4

---

### [AI 맛보기]

📌 전부 패턴과 특징을 찾아내는 것



📌 고차원적인 문제를 풀어내는 것 

   - 인공지능 (가장 상위 개념)
   - 머신러닝
        - 데이터로부터 특징을 추출, 목적에 맞게 분류하는 훈련을 통해서 데이터를 가장 잘 설명하는 함수를 디자인하는 것
        - 학습 단계, training step : Error, Lost, Cost (예측의 결과가 수치로 나옴) -> 에러 값을 기반으로 모델을 수정해 나가는 방식
        - Prediction : 학습된 모델의 예측 성능 (예측값)을 확인하는 방법
             - loss(오차의 크기) = prediction (가정한 함수) - truth (실제 데이터)
             - loss 값이 클수록 실제 데이터 분포와 차이가 심한 것
             - Mean Square Error, 평균 제곱 오차
                  - 모든 x값에 대해 각각의 loss 값을 계산
                  - 이 값들의 제곱 합의 평균을 구함
                  - 전체 데이터를 아우르는 loss를 구할 수 있음
             - loss 값이 크면 가중치를 더 크게 조정, 작으면 작게 조정 -> loss 값이 작아졌다면 가중치 조정을 멈춤
             - loss 값이 적게 나올때까지 반복하여 작업을 진행함
   - 딥러닝 (가장 하위 개념)

📌 학습 방식

- 지도 학습, Supervised Learning
     - 문제가 주어지고, 문제에 대한 정답도 다 알려주는 것
     - 답이 나오는 과정을 학습하는 것
     - Classification (분류) | Regression (값으로 나오는 것)
     - input → **encoder** (특징을 찾아내는 것) → feature → **decoder** (정답을 도출) → output

     *※  그룹화 : 데이터의 특징을 분석하여 유사한 데이터들을 모아 같은 그룹으로 묶는 것 ※*

- 비지도 학습, Unsupervised Learning
     - 문제가 주어지고, 정답은 너가 알아서 찾아라.
     - Clustering (특징을 통한 군집화) | Dimension Reduction

     *※ 지도 학습과 비지도 학습을 구분하는 것 : Label(정답)의 유무 ※*

- 세미-지도학습, Semi-Supervised Learning
     - 문제는 있는데 답이 정해져 있는 것도 있고, 답이 주어지지 않은 경우도 있음.
     - (비중은 없음)
     - Classification | Regression
 
- 강화 학습, Reinforcement Learning
     - 문제를 일단 주고 인공지능이 답을 가져다주면, 최대 점수를 찾을 수 있도록 하는 것

---

### [조건부 확률]

- `P(B|A)` : B given A : A가 주어졌을 때 B의 조건부 확률 (단, P(A) > 0 )
    
    $$
    P(B|A)=\frac{n(A|B)}{n(A)}=\frac{P(A \cap B)}{P(A)}\\
    P(A \cap B) = P(B|A) \cdot P(A)
    $$
    
- `P(A|B)` : A given B : B가 주어졌을 때 A의 조건부 확률 (단, P(B) > 0 )
    
    $$
    P(A|B) = \frac{n(B|A)}{n(B)}=\frac{P(A \cap B)}{P(B)}\\
    P(A \cap B) = P(A|B) \cdot P(B)
    $$



