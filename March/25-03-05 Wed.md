# 25.03.05.Wed - TIL

### [오늘 할 일]

- 데일리 스크럼
- 일정 정리
- 금일 강의 내용 복습 및 정리
- 해커톤 내용 정리 및 회고 작성
- 자소서 작성

---

## [학습 내용]

### 1. 조기 중단, Early Stopping

-> 기계 학습에서 경사 하강법과 같은 반복 방법, 학습자를 훈련할 때 과적합을 방지하기 위해 사용되는 정규화의 형태

*※ 정규화 (Regularization) : 머신러닝과 딥러닝 모델에서 과적합을 방지하고 모델의 일반화 성능을 향상시키기 위해 사용되는 다양한 기술 ※* 

- 모델이 과적합되는 것을 방지하고, 학습 시간을 줄이며, 최적의 성능을 보이는 지점을 찾아 **학습을 조기에 멈추기 위해서** 사용
- 훈련 중 검증 손실을 모니터링하다가, 일정 에포크 동안 개선되지 않으면 학습을 자동으로 중단하도록 설정
- 데이터 전처리 vs 얼리 스탑핑
    
    
    |  | 데이터 전처리 | 얼리 스탑핑 |
    | --- | --- | --- |
    | 목적 | **학습하기 전**에 데이터를 정리하고 준비하는 과정 | **학습 도중**에 모델의 과적합을 방지하고 불필요한 학습을 피하기 위해 **학습을 조기 종료**하는 기법 |
    | 방법 | 데이터 정제, 변환, 분할 등 | 모델의 성능이 일정 기간 동안 개선되지 않으면 학습 중지 → 일반적으로 **검증 손실이나 검증 정확도를 모니터링**  |

---

### 2. 하이퍼파라미터 조정, Hyperparameter Tuning

-> 머신 러닝 모델의 성능을 최적화하기 위해 모델의 파라미터 값을 조정하는 과정

-> 사전 훈련 모델의 세부 설정을 바꿀 수 있는 중요한 방법 중 하나로 사용됨.

- 하이퍼파라미터 종류
    
    
    | 이름 | 내용 | 예시 |
    | --- | --- | --- |
    | 학습 속도(Learning Rate) | 가중치 업데이트의 크기를 결정하는 값 | 0.01, 0.001, 0.0001 |
    | 배치 크기(Batch Size) | 한 번에 학습할 데이터 샘플의 개수 | 32, 64, 128 |
    | 에포크(Epochs) | 전체 데이터셋을 몇 번 반복하여 학습할지를 결정 | 10, 50, 100 |
    | 옵티마이저(Optimizer) | 모델 학습 과정을 최적화하는 알고리즘 선택 | SGD, **Adam**, RMSprop |
    | 모멘텀(Momentum) | SGD와 같은 옵티마이저에서 기울기 업데이트에 대한 관성을 추가하여 학습을 가속화하고 안정화하는 파라미터 | 0.9, 0.95, 0.99 |
    | 드롭아웃 비율(Dropout Rate) | 학습 과정에서 무작위로 뉴런을 끄는 비율 | 0.2, 0.5 |
    | 정규화 파라미터(Regularization Parameter) | 과적합을 방지하기 위해 가중치에 패널티를 부여하는 값 | L2 정규화(lambda) 값: 0.01, 0.001 |
    | 학습률 감쇠(Learning Rate Decay) | 학습이 진행됨에 따라 학습률을 감소시키는 방법 | 0.1, 0.01, 0.001 |
    | 활성화 함수(Activation Function) | 각 뉴런의 출력값을 결정하는 함수 | ReLU, Sigmoid, Tanh |
    | 초기화 방법(Initialization Method) | 가중치 초기화 방법 | He, Glorot, Random |
    - 다양한 하이퍼파라미터 조합을 탐색하고 평가해보며 모델의 성능이 좋아지도록 최적화
- 모델 학습 코드 일부
    
    ```python
    model = create_model(learning_rate=0.001)
    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)
    ```
    
    - learning_rate, X_train, y_train, epochs, batch_size, validation_split : (여기서의) 하이퍼파라미터
    - 하이퍼파라미터는 개발자가 임의의 값을 할당하면서 조정 가능

---
